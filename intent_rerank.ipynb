{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 8394eca39f743cf4cd098fb92cc4718d34be1d04 9.11290\n",
      "\n",
      " 2 d8ed29985999ca9642e7940fbf7484f544df54e5 7.17950\n",
      "\n",
      " 3 1e21c514f89375098dec5b947aa5f6bcdd0377c5 6.05780\n",
      "\n",
      " 4 6258323b0ddedd0892febb36c1772a10820e0b0c 6.01710\n",
      "\n",
      " 5 4f6b7f8daa322801887b2ef8c2c14788e607e3b8 5.96180\n",
      "\n",
      " 6 20c7139595570f080fc85a054c84262d11a488bd 5.95310\n",
      "\n",
      " 7 34c84bc46dbac879d82b675d2adf2b7f1911d387 5.75280\n",
      "\n",
      " 8 66ea9389b7ecd7f8070cddc8c6c3ecbf301e3577 5.73630\n",
      "\n",
      " 9 24ff5027e7042aeead47ef3071f1a023243078bb 5.60510\n",
      "\n",
      "10 511921e775ab05a1ab0770a63e57c93da51c8526 5.58010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:00<00:00, 44.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id = 8394eca39f743cf4cd098fb92cc4718d34be1d04\n",
      "contents = Stakeholders in Explainable AI There is general consensus that it is important for artificial intelligence (AI) and machine learning systems to be explainable and/or interpretable. However, there is no general consensus over what is meant by ‘explainable’ and ‘interpretable’. In this paper, we argue that this lack of consensus is due to there being several distinct stakeholder communities. We note that, while the concerns of the individual communities are broadly compatible, they are not identical, which gives rise to different intents and requirements for explainability/interpretability. We use the software engineering distinction between validation and verification, and the epistemological distinctions between knowns/unknowns, to tease apart the concerns of the stakeholder communities and highlight the areas where their foci overlap or diverge. It is not the purpose of the authors of this paper to ‘take sides’ — we count ourselves as members, to varying degrees, of multiple communities — but rather to help disambiguate what stakeholders mean when they ask ‘Why?’ of an AI.\n",
      "doc_id = 34c84bc46dbac879d82b675d2adf2b7f1911d387\n",
      "contents = DARPA's explainable artificial intelligence (XAI) program The DARPA's Explainable Artificial Intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. This talk will summarize the XAI program and present highlights from these Phase 1 evaluations.\n",
      "doc_id = d8ed29985999ca9642e7940fbf7484f544df54e5\n",
      "contents = Explainable , Normative , and Justified Agency In this paper, we pose a new challenge for AI researchers – to develop intelligent systems that support justified agency. We illustrate this ability with examples and relate it to two more basic topics that are receiving increased attention – agents that explain their decisions and ones that follow societal norms. In each case, we describe the target abilities, consider design alternatives, note some open questions, and review prior research. After this, we return to justified agency, offering a hypothesis about its relation to explanatory and normative behavior. We conclude by proposing testbeds and experiments to evaluate this empirical claim and encouraging other researchers to contribute to this crucial area. 1 Background and Motivation Autonomous artifacts, from self-driving cars to drones to household robots, are becoming more widely adopted, and this trend seems likely to accelerate in the future. The increasing reliance on these devices has raised concerns about our ability to understand their behavior and our capacity to ensure their safety. Before intelligent agents can gain widespred acceptance in society, they must be able to communicate their decision making to humans in ways that convince us they actually share our aims. This challenge involves two distinct but complementary issues. The first is the need for agents to explain the reasons they carried out particular courses of action in terms that we can understand. Langley et al. (2017) have referred to this as explainable agency. The second is the need for assurance that, when agents pursue explicit goals, they will also follow the many implicit rules of society. We will refer to this ability as normative agency. Both of these functions are necessary underpinnings of trustable autonomous systems, and two capabilities are closely intertwined. Consider a scenario in which a person drives a friend with a ruptured appendix to the hospital. The driver exceeds the speed limit, weaves in and out of traffic, runs through red lights, and even drives on a sidewalk, although he is still careful to avoid hitting other cars or losing control on turns. Afterwards, the driver explains that he took such drastic actions because he thought the passenger’s life was in danger, Copyright c © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. so reaching the hospital in short order had higher priority than being polite to others along the way or obeying traffic laws. Humans can defend their behavior in this manner even when they violate serious societal norms, and we want intelligent agents of the future to exhibit the same ability to justify their decisions and actions on demand. We discuss this challenge in the sections that follow, arguing that such justified agency is an important topic that deserves substantial attention from AI researchers. We first examine the two related topics of explainable agency and normative agency. In each case, we describe the desired abilities, offer illustrative examples, and touch on relevant research. We also consider the space of agent designs, including some open issues that require additional work. After this, we turn to justified agency, arguing that it combines the first two abilities and proposing a hypothesis about what else is required to support it. We close by considering some testbeds and experiments that would let the research community evaluate progress in this critical arena. 2 Explainable Agency People can usually explain their reasons for making decisions and taking actions. When someone purchases a microwave oven, rearranges furniture in a room, or plans a vacation, he can state the choices considered, why he selected one alternative over the others, and even how his decision might have differed in other circumstances. Retrospective reports are seldom perfect, but they often offer important windows into the decision process. As intelligent agents become both more autonomous and more prevalent, it is essential that they support similar abilities. We will say that: • An intelligent system exhibits explainable agency if it can provide, on request, the reasons for its activities. Consider some examples from the realm of autonomous vehicles. If we ask a self-driving car why it followed a given route, it should be able to state that the path had few lights and stop signs while still being reasonably short. More importantly, when we ask the vehicle why it swerved into another car, it should explain that it was the only way to avoid hitting an unexpected jaywalker. Like humans, such agents should have reasons for their actions and they should be able to communicate them to others when asked. Tackling this problem requires that we make design decisions about representations and processes needed to support explainable agency, some of which we will borrow from Langley et al.’s (2017) analysis. One issue concerns what will count as legitimate explanations. Should plausible post hoc rationalizations be acceptable if an agent’s decisionmaking procedures are not interpretable or should we require genuine insights into why the agent took its actions? We argue that only the latter should be viewed as reasonable accounts, which implies that the agents should make decisions in ways that are transparent and easily communicated. There are many well-established methods in the AI arsenal that meet this criterion, but opaque policies induced from large training sets will not suffice. Another design choice involves whether to explain activities in terms of individual actions or higher-level structures. Research in machine learning has emphasized reactive control (e.g., Erwig et al., 2018), which supports the first alternative, whereas AI planning systems make choices about entire sequences of actions. We predict that humans will find plan-oriented explanations more familiar and easier to understand, and thus offer a natural approach to adopt. Even when a plan goes awry during execution, an agent can still give the reasons it decided to change course and how its new plan responded to the surprise. Some frameworks, like hierarchical task networks, also specify plans at multiple levels of abstraction, which would let a system offer more or less detailed explanations, down to individual actions if desired. Any intelligent agent must use information to make decisions about which activities to pursue, and a third issue concerns how to encode this content. One response, widely adopted in AI planning research, relies on symbolic goals, often stated as logical expressions. Another option, popular in game-playing systems, instead uses numeric evaluation functions that comprise sets of weighted features. Each approach has advantages for explainable agency: goals provide explicit end points for chains of actions, while functions show how such plans handle trade offs. Although these typically appear in isolation, they can also be combined. For instance, Langley et al. (2016) describe an agent architecture that associates functions with goals, using their weighted sum to guide planning. Such hybrid frameworks offer one promising approach to building explainable agents. Within this design space, we still need research on a number of open issues about explainable agency: These include extending intelligent systems to: • Generate explanatory content. When deciding on courses of action, an agent must consider different alternatives, evaluate them, and select one of the options to pursue. This should take place during generation of plans and during their execution, producing traces that can be used in later explanations of the agent’s activities. • Store generated content. As it makes these decisions, an agent must cache information about the choices that it considered and the reasons that it favored one over the others, in an episodic memory or similar repository. This requires not just retaining the content about decisions, but also indexing it in ways that support later access. • Retrieve stored content. After it has completed an activity, an agent must be able to retrieve decision traces that are relevant to different types of questions. This requires transforming the queries into cues and using them to access content in episodic memory about alternatives considered, their evaluations, and the final choices made. • Communicate retrieved content. Once it has retrieved episodic traces in response to a question, an agent must identify those aspects most relevant to the query, translate them into an understandable form, and share the answer. This should include no more detail than needed to convey the reasons for making the decision under examination. Research on analogical planning (e.g., Jones and Langley, 2005; Veloso et al., 1995) has addressed issues of storage, indexing, and retrieval, but not for the purpose of self report. Leake (1992) presented a computational theory of explanation, but it focused on accounts of other agents’ behaviors. The AI literature also includes other research relevant to this topic. Early explanation systems recorded inference chains and recounted them when asked to justify their conclusions (Swartout, Paris, and Moore, 1991), with some systems supporting higher-level accounts with meta-level rules (Clancey, 1983), but these did not deal with physical activities. More relevant work comes from Johnson (1994) and van Lent et al. (2004), who developed agents for military mssions that recorded their decisions, offered reasons on request, and anwered counterfactual queries. However, they dealt with knowledge-guided execution rather than agentgenerated plans and, despite linking actions to objectives, did not state why some activities were preferable to others. In more recent work, Briggs and Scheutz (2015) have reported an interactive robot that gives reasons why it cannot carry out some task, drawing on five explanation types, including lack of knowledge and physical ability. Th\n",
      "doc_id = 4f6b7f8daa322801887b2ef8c2c14788e607e3b8\n",
      "contents = There is a blind spot in AI research advances in the technical domains of AI. Alongside such efforts, designers and researchers from a range of disciplines need to conduct what we call social-systems analyses of AI. They need to assess the impact of technologies on their social, cultural and political settings. A social-systems approach could investigate, for instance, how the app AiCure — which tracks patients’ adherence to taking prescribed medication and transmits records to physicians — is changing the doctor– patient relationship. Such an approach to perform a range of complex tasks in everyday life. These ranged from the identification of skin alterations that are indicative of earlystage cancer to the reduction of energy costs for data centres. The workshops also highlighted a major blind spot in thinking about AI. Autonomous systems are already deployed in our most crucial social institutions, from hospitals to courtrooms. Yet there are no agreed methods to assess the sustained effects of such applications on human populations. Recent years have brought extraordinary There is a blind spot in AI research\n",
      "doc_id = 24ff5027e7042aeead47ef3071f1a023243078bb\n",
      "contents = Optimizing Space-Air-Ground Integrated Networks by Artificial Intelligence It is widely acknowledged that the development of traditional terrestrial communication technologies cannot provide all users with fair and high quality services due to the scarce network resource and limited coverage areas. To complement the terrestrial connection, especially for users in rural, disasterstricken, or other difficult-to-serve areas, satellites, unmanned aerial vehicles (UAVs), and balloons have been utilized to relay the communication signals. On the basis, Space-Air-Ground Integrated Networks (SAGINs) have been proposed to improve the users’ Quality of Experience (QoE). However, compared with existing networks such as ad hoc networks and cellular networks, the SAGINs are much more complex due to the various characteristics of three network segments. To improve the performance of SAGINs, researchers are facing many unprecedented challenges. In this paper, we propose the Artificial Intelligence (AI) technique to optimize the SAGINs, as the AI technique has shown its predominant advantages in many applications. We first analyze several main challenges of SAGINs and explain how these problems can be solved by AI. Then, we consider the satellite traffic balance as an example and propose a deep learning based method to improve the traffic control performance. Simulation results evaluate that the deep learning technique can be an efficient tool to improve the performance of SAGINs.\n",
      "doc_id = 6258323b0ddedd0892febb36c1772a10820e0b0c\n",
      "contents = Using Ontological Engineering to Overcome Common AI-ED Problems This paper discusses long-term prospects of AI-ED research with the aim of giving a clear view of what we need for further promotion of the research from both the AI and ED points of view. An analysis of the current status of AI-ED research is done in the light of intelligence, conceptualization, standardization and theory-awareness. Following this, an ontology-based architecture with appropriate ontologies is proposed. Ontological engineering of IS/ID is next discussed followed by a road map towards an ontology-aware authoring system. Heuristic design patterns and XML-based documentation are also discussed.\n",
      "doc_id = 1e21c514f89375098dec5b947aa5f6bcdd0377c5\n",
      "contents = Adaptation in natural and artificial systems Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)\n",
      "doc_id = 511921e775ab05a1ab0770a63e57c93da51c8526\n",
      "contents = Use of AI Techniques for Residential Fire Detection in Wireless Sensor Networks Early residential fire detection is important for prompt extinguishing and reducing damages and life losses. To detect fire, one or a combination of sensors and a detection algorithm are needed. The sensors might be part of a wireless sensor network (WSN) or work independently. The previous research in the area of fire detection using WSN has paid little or no attention to investigate the optimal set of sensors as well as use of learning mechanisms and Artificial Intelligence (AI) techniques. They have only made some assumptions on what might be considered as appropriate sensor or an arbitrary AI technique has been used. By closing the gap between traditional fire detection techniques and modern wireless sensor network capabilities, in this paper we present a guideline on choosing the most optimal sensor combinations for accurate residential fire detection. Additionally, applicability of a feed forward neural network (FFNN) and Naïve Bayes Classifier is investigated and results in terms of detection rate and computational complexity are analyzed.\n",
      "doc_id = 66ea9389b7ecd7f8070cddc8c6c3ecbf301e3577\n",
      "contents = The turing test track of the 2012 Mario AI Championship: Entries and evaluation The Turing Test Track of the Mario AI Championship focused on developing human-like controllers for a clone of the popular game Super Mario Bros. Competitors participated by submitting AI agents that imitate human playing style. This paper presents the rules of the competition, the software used, the voting interface, the scoring procedure, the submitted controllers and the recent results of the competition for the year 2012. We also discuss what can be learnt from this competition in terms of believability in platform games. The discussion is supported by a statistical analysis of behavioural similarities and differences among the agents, and between agents and humans. The paper is co-authored by the organizers of the competition (the first three authors) and the competitors.\n",
      "doc_id = 20c7139595570f080fc85a054c84262d11a488bd\n",
      "contents = The 2009 Mario AI Competition This paper describes the 2009 Mario AI Competition, which was run in association with the IEEE Games Innovation Conference and the IEEE Symposium on Computational Intelligence and Games. The focus of the competition was on developing controllers that could play a version of Super Mario Bros as well as possible. We describe the motivations for holding this competition, the challenges associated with developing artificial intelligence for platform games, the software and API developed for the competition, the competition rules and organization, the submitted controllers and the results. We conclude the paper by discussing what the outcomes of the competition can teach us both about developing platform game AI and about organizing game AI competitions. The first two authors are the organizers of the competition, while the third author is the winner of the competition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import json\n",
    "import numpy as np\n",
    "from intent_exs import IntentEXS\n",
    "from pyserini.search.lucene import LuceneSearcher  \n",
    "\n",
    "index_path = '/home/wang/xaiss/datasets/scidocs/corpus_index'\n",
    "query = 'what is explainable AI?'\n",
    "\n",
    "searcher = LuceneSearcher(index_path)   # load a searcher from pre-computed index.\n",
    "\n",
    "hits = searcher.search(query)\n",
    "# Print the first 10 hits:\n",
    "for i in range(0, 10):\n",
    "    print(f'{i+1:2} {hits[i].docid:15} {hits[i].score:.5f}')\n",
    "    print()\n",
    "# extract the retrieved doc ids and doc contents.\n",
    "doc_ids = [hit.docid for hit in hits]\n",
    "docs = dict([(hit.docid, json.loads(searcher.doc(hit.docid).raw())['contents']) for hit in hits])\n",
    "\n",
    "# Load a reranking model\n",
    "from beir.reranking.models import CrossEncoder\n",
    "model = 'cross-encoder/ms-marco-electra-base'\n",
    "reranker = CrossEncoder(model)\n",
    "\n",
    "# build query-doc pair for reranking model as input.\n",
    "sentence_pairs = []\n",
    "for doc_id in doc_ids:\n",
    "    doc_text = docs[doc_id]\n",
    "    sentence_pairs.append([query, doc_text])\n",
    "rerank_scores = reranker.predict(sentence_pairs, batch_size=1)\n",
    "\n",
    "# show reranked docs.\n",
    "reranked_docids = np.array(doc_ids)[np.argsort(rerank_scores)[::-1]]\n",
    "for i, doc_id in enumerate(reranked_docids):\n",
    "    print('doc_id =', doc_id)\n",
    "    #if i < 1:\n",
    "    print('contents =', docs[doc_id])\n",
    "\n",
    "\n",
    "    \n",
    "# build corpus for IntentEXS explain function\n",
    "corpus = {'query': query,\n",
    "        'scores': dict([(doc_id, score) for doc_id, score in zip(doc_ids, rerank_scores)]),\n",
    "        'docs': docs\n",
    "}\n",
    "params = {'top_idf': 10, 'topk': 5, 'max_pair': 100, 'max_intent': 10, 'style': 'random'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked!\n",
      "Picked!\n",
      "Picked!\n",
      "Picked!\n",
      "Picked!\n",
      "Picked!\n",
      "Picked!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['explain', 'range', 'use', 'key', 'architecture', 'stakeholder', 'summarize']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init the IntentEXS object.\n",
    "Intent = IntentEXS(reranker, index_path, 'bm25')\n",
    "expansion = Intent.explain(corpus, params)\n",
    "expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: scidocs\n",
    "query: what is CRONA?\n",
    "content: ['provide', 'kind', 'what', 'guide', 'article']\n",
    "content + title: ['what','employ','cognitive','business','describe', 'accurate', 'become','application']\n",
    "title: ['what', 'make', 'ontology', 'function', 'about', 'from', 'marketing', 'base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('xaiss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd813641fab82437fc5ddf0581495027ff56177db175bb561b469bfb0d2585e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
